---
title: "Hate Speech Detection"
output:
  html_document:
    df_print: paged
---

## Imports and constants
```{r}
library(keras)
library(utf8)
library(tm)

batch_size <- 128
epochs <- 3

```

## Functions
```{r}


clean_data <- function(data){
  cat("--> Transforming text to lower case...\n")
  data <- tolower(data)
  cat("--> Done.\n")
  cat("--> Checking utf8 encoding and NFC...\n")
  basic_check(data)
  cat("--> Removing unwanted words and/or characters...\n")
  data <- remove_unwanted_ch_nl_and_spaces(data)
  cat("--> Done.\n")
  
  return(data)
}

remove_unwanted_ch_nl_and_spaces <- function(data){
  ## Remove usernames (i.e. words starting with '@')
  data <- gsub(pattern = "@\\w*", " ", data )
  
  ## Remove hashtags
  data <- gsub(pattern = "#"," ", data)
  
  ## Remove word "RT", indicating re-tweet
  data <- gsub(patter = "\\srt\\s|^rt\\s|\\srt$", " ", data)
  
  ## Remove other characters
  data_wo_unwanted_ch <- gsub("([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)|http.+?", " ",data)
  
  ## Remove single letters
  data_wo_single_ch <- gsub("\\s+[a-zA-Z]\\s"," ",data_wo_unwanted_ch)
  
  ## Remove \n from the text  
  data_wo_nl <- gsub("[\n]{1,}", " ", data_wo_single_ch)
  
  ## remove sequences of more than one space
  data <- gsub("[ ]{2,}"," ",data_wo_nl)
  
  ## Remove starting and ending spaces
  data <- gsub("(\\s$)|(^\\s)", "", data)
  
  ## Stemming 
  data <- stemDocument(data,language="en")
  
  return(data)
  
}

basic_check <- function(data){
  # Check encoding
  test1 <- data[!utf8_valid(data)]
  
  #Check character normalization. Specifically, the normalized composed form (NFC)
  data_q_nfc <- utf8_normalize(data)
  test2 <- sum(data_q_nfc != data)
  
  if (identical(test1, character(0)) && test2 == 0){
    cat("--> Encoding and NFC check passed.\n")
  }
  else{
    cat("--> Error: Check not passed\nEncoding result: ",test1,"\nCharacter normalization result: ",test2, "\n")
  }
}

tokenize_and_pad <- function(data, max_features = 30000, max_length = 100){
  token <- text_tokenizer(max_features) %>%     fit_text_tokenizer(unlist(data['tweet']))
  text <- texts_to_sequences(token,unlist(data['tweet']))
  text <- pad_sequences(text,max_length) 
  return (text)
}

split_data <- function(text,data){
  set.seed(101)
  sample <- sample(nrow(text), size = floor(.80*nrow(data)), replace = F)
  x_train <- text[sample,]
  x_test <- text[-sample,]
  y_train <- data[sample,]$label
  y_test <- data[-sample,]$label
  return (list(x_train, x_test, y_train, y_test))
}

build_model <- function(max_features = 30000, max_length = 100, output_dim = 128, lstm_units = 64, dropout_rate = 0.5){
  model <- keras_model_sequential()
  model %>%
  layer_embedding(input_dim = max_features,
                  output_dim = output_dim,
                  input_length = max_length) %>%
  bidirectional(layer_lstm(units = lstm_units)) %>%
  layer_dropout(rate = dropout_rate) %>%
  layer_dense(units = 1, activation = 'sigmoid')

  model %>% compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = c('accuracy'))
  return(model)
}
```
## Metrics
```{r}

recall<- function(CM){
    TP <- CM[4]
    FN <- CM[2]
    rec <- TP/(TP+FN)
    return(rec)
}

precision <- function(CM){
    TP <- CM[4]
    FP <- CM[3]
    prec <- TP/(TP+FP)
    return(prec)
}

f1 <- function(CM){
  prec <- precision(CM)
  rec <- recall(CM)
  res <- 2 * (prec * rec)/ (prec+rec)
  return (res)
}

print_metrics <- function(predictions, y_target){

  y_pred <- ifelse(predictions >0.5,1,0)
  eq <- y_pred == y_target
  
  acc <- sum(eq)/length(eq)
  
  # Compute Confusion Matrix
  CM = table(y_target,y_pred)

  
  f1s <- f1(CM)
  prec <- precision(CM)
  rec <- recall(CM)

  cat("\nAccuracy",format(acc*100, nsmall=2,digits = 4),"%\n")
  cat("precision",format(prec*100, nsmall=2,digits = 3),"%\n")
  cat("recall",format(rec*100, nsmall=2,digits = 3),"%\n")
  cat("f1 score",format(f1s*100, nsmall=2,digits = 3),"%\n\n")
  cat("Confusion Matrix\n")
  print(CM)
  
}
```


## Load Dataset
```{r}

data1 <- read.csv("./Data/train.csv")
data1 <- data1[-c(1)]    #drop id column

data2 <- read.csv("./Data/train2.csv", sep = ";")
data2$label <- ifelse(data2$label >0.5,1,0)

data <- rbind(data1,data2)

head(data['tweet'],n=5)
```

## Check and clean text data
```{r}
data['tweet'] <- lapply(data['tweet'],function(x) clean_data(x))
head(data['tweet'],n=5)
```

## Tokenize
```{r,message=FALSE}
text <- tokenize_and_pad(data)
```

## Split training and test set
```{r}
splitList <- split_data(text,data)
x_train <- splitList[[1]]
x_test <- splitList[[2]]
y_train <- splitList[[3]]
y_test <- splitList[[4]]
```


## Build model
```{r, message=FALSE}
model <- build_model()
```

## Train
```{r}
history <- model %>% fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split =0.2)
plot(history)
```

## Predict 
```{r}
predictions <- model %>% predict(x_test)

print_metrics(predictions,y_test)

```
## Considerations 
False positives are too much, the model predicts most of the true samples as negative ones. The reason is that our dataset mostly consists of negative samples. To balance classes we can use oversampling techinique.

## Balance classes

```{r}

#extract train set
sample <- sample(nrow(data), size = floor(.80*nrow(data)))
train <- data[sample,]

# keep test set apart
test <- data[-sample,]

train_min <- train[train$label == 1,]
train_maj <- train[train$label == 0,]

# create the new set for class 1 sampling rows from class 1 for "nrow(train_maj)" times 
train_min_oversampled <- train_min[sample(nrow(train_min),length(train_maj$label), replace = TRUE),]

cat("Class 1 size: ", dim(train_min_oversampled),"\n")
cat("Class 0 size: ", dim(train_maj), "\n")

# concatenate the dataframes
data_over <- rbind(train_min_oversampled,train_maj)

# shuffle the new dataset
data_over <- data_over[sample(1:nrow(data_over)),]
cat("Training Dataset size: ", dim(data_over),"\n")

```

## Tokenize, Split, Build Model, Train, Predict
```{r}

# recombine the dataset (train+test) and tokenize
split_idx = dim(data_over)[1]
data_over_full <- rbind(data_over,test)
end = dim(data_over_full)[1]

text_over <- tokenize_and_pad(data_over_full)

x_train_over <- text_over[1:split_idx,]
y_train_over <- data_over_full$label[1:split_idx]
x_test <- text_over[split_idx:end,]
y_test <- data_over_full$label[split_idx:end]

model <- build_model()


history <- model %>% fit(x_train_over, y_train_over, batch_size = batch_size, epochs = epochs, validation_split = 0.2)
plot(history)

cat("Training with Batch size = ",batch_size, " Epochs = ",epochs)
predictions <- model %>% predict(x_test)
print_metrics(predictions,y_test)


```


